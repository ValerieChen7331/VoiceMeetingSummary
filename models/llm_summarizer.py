from langchain_openai import ChatOpenAI
from config import Config
from langchain_community.llms import Ollama

# ğŸ”¹ **LLM æ‘˜è¦ç”Ÿæˆ**
class LLMTextSummarizer:
    """ä½¿ç”¨ LLM ç”Ÿæˆæ–‡æœ¬æ‘˜è¦"""
    @staticmethod
    def summary_generator(chunks, prompt_template):
        """
        ä½¿ç”¨ LLM å°æ–‡æœ¬åˆ†æ®µé€²è¡Œæ‘˜è¦ç”Ÿæˆ

        åƒæ•¸ï¼š
        chunks (list): åˆ†æ®µçš„æ–‡æœ¬åˆ—è¡¨
        prompt_template (str): æ‘˜è¦æç¤ºèªæ¨¡æ¿ï¼Œéœ€åŒ…å« {context} å ä½ç¬¦

        å›å‚³ï¼š
        str: æ‰€æœ‰æ®µè½æ‘˜è¦çš„åˆä½µçµæœ
        """
        # åˆå§‹åŒ– LLM æ¨¡å‹ (é€™è£¡ä½¿ç”¨çš„æ˜¯ Ollama è‡ªæ¶æ¨¡å‹)
        # model = ChatOpenAI(
        #     api_key="ollama",
        #     model=Config.LLM_MODEL_NAME,
        #     base_url=Config.LLM_API_BASE_URL
        # )
        model = Ollama(base_url=Config.LLM_API_BASE_URL, model=Config.LLM_MODEL_NAME)
        summaries = []

        # é€æ®µè™•ç†æ–‡æœ¬
        for chunk in chunks:
            # å°‡ç•¶å‰æ®µè½å¸¶å…¥æç¤ºæ¨¡æ¿ä¸­
            prompt = prompt_template.format(context=chunk)
            # å‘¼å« LLM æ¨¡å‹é€²è¡Œæ‘˜è¦ç”Ÿæˆ
            response = model.invoke(prompt)
            # å¦‚æœæ¨¡å‹æœ‰å›æ‡‰ï¼Œä¸”å…§å®¹å­˜åœ¨ï¼ŒåŠ å…¥æ‘˜è¦çµæœ
            if response and response.content:
                summaries.append(response.content)

        # å°‡æ‰€æœ‰æ‘˜è¦æ®µè½åˆä½µç‚ºå–®ä¸€æ–‡æœ¬ä¸¦å›å‚³
        return "\n".join(summaries)
