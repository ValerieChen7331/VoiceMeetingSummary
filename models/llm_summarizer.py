from langchain_openai import ChatOpenAI
from config import Config
from langchain_community.llms import Ollama

# ğŸ”¹ **LLM æ‘˜è¦ç”Ÿæˆ**
class LLMTextSummarizer:
    """ä½¿ç”¨ LLM ç”Ÿæˆæ–‡æœ¬æ‘˜è¦"""
    @staticmethod
    def summary_generator(chunks, prompt):
        """
        ä½¿ç”¨ LLM å°æ–‡æœ¬åˆ†æ®µé€²è¡Œæ‘˜è¦ç”Ÿæˆ

        åƒæ•¸ï¼š
        chunks (list): åˆ†æ®µçš„æ–‡æœ¬åˆ—è¡¨
        prompt_template (str): æ‘˜è¦æç¤ºèªæ¨¡æ¿ï¼Œéœ€åŒ…å« {context} å ä½ç¬¦

        å›å‚³ï¼š
        str: æ‰€æœ‰æ®µè½æ‘˜è¦çš„åˆä½µçµæœ
        """
        prompt_template = prompt + "é€å­—ç¨¿ï¼š{context}"
        # åˆå§‹åŒ– LLM æ¨¡å‹ï¼ˆé€™è£¡ä½¿ç”¨çš„æ˜¯ Ollama è‡ªæ¶æ¨¡å‹ï¼‰
        model = Ollama(base_url=Config.LLM_API_BASE_URL, model=Config.LLM_MODEL_NAME)
        summaries = []

        # é€æ®µè™•ç†æ–‡æœ¬
        for chunk in chunks:
            prompt = prompt_template.format(context=chunk)
            response = model.invoke(prompt)
            if response:
                summaries.append(response)

        return "\n".join(summaries)
