# ğŸ”¹ **åŒ¯å…¥å¿…è¦å¥—ä»¶**
import streamlit as st
import os
import re
import base64
import tempfile
from io import BytesIO
from docxtpl import DocxTemplate
from langchain_openai import ChatOpenAI
import whisper
import subprocess

# ğŸ”¹ **é é¢åŸºæœ¬è¨­å®š**
st.set_page_config(page_title="Meeting Summary Assistant", layout="wide")


# ğŸ”¹ **æ‡‰ç”¨ç¨‹å¼è¨­å®š**
class Config:
    LLM_MODEL_NAME = "cwchang/llama-3-taiwan-8b-instruct-dpo:f16"
    LLM_API_BASE_URL = "http://10.5.61.81:11436/v1"
    BACKGROUND_IMAGE_PATH = "bg.jpg"
    OUTPUT_FOLDER = "summaries"
    DEFAULT_USER_ID = "guest"
    DOCX_MIME_TYPE = "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    AUDIO_MODEL_TYPE = "base"


# ğŸ”¹ **Whisper éŸ³æª”è½‰éŒ„**
class AudioTranscriber:
    """Whisper éŸ³æª”è½‰éŒ„å™¨ (æ”¯æ´ m4aã€wavã€mp3ã€mp4)"""

    _whisper_model = None

    @classmethod
    def load_whisper_model(cls):
        """è¼‰å…¥ Whisper æ¨¡å‹ (Lazy Load)"""
        if cls._whisper_model is None:
            try:
                cls._whisper_model = whisper.load_model(Config.AUDIO_MODEL_TYPE)
            except Exception as error:
                st.error(f"âŒ Whisper æ¨¡å‹è¼‰å…¥å¤±æ•—: {error}")
                return None
        return cls._whisper_model

    @classmethod
    def transcribe(cls, uploaded_audio_file):
        """å°‡éŸ³æª”è½‰éŒ„ç‚ºé€å­—ç¨¿"""
        whisper_model = cls.load_whisper_model()
        if whisper_model is None:
            return ""

        if uploaded_audio_file.size == 0:
            st.error("âš ï¸ éŸ³æª”ç‚ºç©ºï¼Œè«‹é‡æ–°ä¸Šå‚³ã€‚")
            return ""

        file_extension = os.path.splitext(uploaded_audio_file.name)[-1].lower()
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_audio:
            temp_audio.write(uploaded_audio_file.getbuffer())
            temp_audio_path = temp_audio.name

        try:
            wav_audio_path = cls.convert_to_wav(temp_audio_path, file_extension)
            if not wav_audio_path:
                st.error("âš ï¸ éŸ³æª”è½‰æ›å¤±æ•—ï¼Œè«‹ä¸Šå‚³æœ‰æ•ˆéŸ³æª”ã€‚")
                return ""

            transcript_result = whisper_model.transcribe(wav_audio_path, language="zh")
            return transcript_result.get("text", "")

        except Exception as error:
            st.error(f"âš ï¸ éŸ³æª”è½‰éŒ„å¤±æ•—: {error}")
            return ""

        finally:
            os.remove(temp_audio_path)
            if wav_audio_path and os.path.exists(wav_audio_path):
                os.remove(wav_audio_path)

    @staticmethod
    def convert_to_wav(input_audio_path, file_extension):
        """è½‰æ›éŸ³æª”ç‚º 16kHz å–®è²é“ WAV"""
        output_wav_path = input_audio_path.rsplit(".", 1)[0] + ".wav"

        try:
            conversion_cmd = ["ffmpeg", "-y", "-i", input_audio_path, "-ar", "16000", "-ac", "1", output_wav_path]
            subprocess.run(conversion_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)
            return output_wav_path if os.path.exists(output_wav_path) and os.path.getsize(output_wav_path) > 0 else None
        except Exception as error:
            st.error(f"âš ï¸ éŸ³æª”è½‰æ›å¤±æ•—: {error}")
            return None


# ğŸ”¹ **LLM æ‘˜è¦ç”Ÿæˆ**
class LLMTextSummarizer:
    """ä½¿ç”¨ LLM ç”Ÿæˆæ–‡æœ¬æ‘˜è¦"""

    @staticmethod
    def generate(text_segments, prompt_template):
        """ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦"""
        model = ChatOpenAI(api_key="ollama", model=Config.LLM_MODEL_NAME, base_url=Config.LLM_API_BASE_URL)
        summaries = []

        for segment in text_segments:
            prompt = prompt_template.format(context=segment)
            response = model.invoke(prompt)
            if response and response.content:
                summaries.append(response.content)

        return "\n".join(summaries)


# ğŸ”¹ **Word æª”æ¡ˆç”¢ç”Ÿ**
class WordDocumentGenerator:
    """ç”¢ç”Ÿ Word æ–‡ä»¶"""

    @staticmethod
    def create(summary_text):
        """å»ºç«‹ Word æ–‡ä»¶"""
        doc = DocxTemplate("template.docx")
        cleaned_text = re.sub(r'(#+|\*\*|__|\*|_|\[.*?\]\(.*?\)|[-*])', '', summary_text)
        doc.render({'summary': cleaned_text})

        buffer = BytesIO()
        doc.save(buffer)
        return buffer.getvalue()


# ğŸ”¹ **VTT é€å­—ç¨¿è™•ç†**
class VTTProcessor:
    """è™•ç† VTT æª”æ¡ˆï¼Œæå–ç´”æ–‡å­—"""

    @staticmethod
    def extract(vtt_data):
        """å¾ VTT æª”æ¡ˆæå–ç´”æ–‡å­—å…§å®¹"""
        try:
            lines = vtt_data.decode('utf-8').splitlines()
            return " ".join(
                line.strip() for line in lines if line and "-->" not in line and not line.startswith("WEBVTT"))
        except Exception as error:
            st.error(f"âš ï¸ VTT è§£æå¤±æ•—: {error}")
            return ""


# ğŸ”¹ **æœƒè­°è¨˜éŒ„åŠ©ç†æ‡‰ç”¨**
class MeetingSummaryApp:
    """æœƒè­°è¨˜éŒ„åŠ©ç†æ‡‰ç”¨ç¨‹å¼"""

    OPTIONS = {
        "audio_transcription": ("ğŸ™ï¸ éŸ³æª”è½‰é€å­—ç¨¿", ["m4a", "wav", "mp3", "mp4"]),
        "vtt_summary": ("ğŸ“ é€å­—ç¨¿ (VTT) ç”Ÿæˆæ‘˜è¦", ["vtt"]),
        "audio_summary": ("ğŸ™ï¸+ğŸ“ éŸ³æª”ç”Ÿæˆæ‘˜è¦", ["m4a", "wav", "mp3", "mp4"]),
    }

    def run(self):
        """å•Ÿå‹•æ‡‰ç”¨ç¨‹å¼"""
        self.show_app_header()
        selected_mode, uploaded_file, summary_prompt, is_submitted = self.show_sidebar()

        if not (uploaded_file and is_submitted):
            return

        if selected_mode == "audio_transcription":
            self.process_audio_transcription(uploaded_file)
        elif selected_mode == "vtt_summary":
            extracted_text = VTTProcessor.extract(uploaded_file.getvalue())
            if extracted_text:
                self.generate_summary(extracted_text, summary_prompt, uploaded_file)
        elif selected_mode == "audio_summary":
            transcript_text = self.process_audio_transcription(uploaded_file)
            if transcript_text:
                self.generate_summary(transcript_text.strip(), summary_prompt, uploaded_file)

    def show_app_header(self):
        """é¡¯ç¤ºæ¨™é¡Œèˆ‡ç™»å‡ºæŒ‰éˆ•"""
        st.markdown('<div style="position:fixed; top:20px; right:20px;">', unsafe_allow_html=True)
        if st.button("Log Out"):
            st.session_state.clear()
        st.markdown('</div>', unsafe_allow_html=True)
        st.title("ğŸ”— æœƒè­°è¨˜éŒ„åŠ©ç† - Meeting Summary Assistant ğŸ”—")

    def show_sidebar(self):
        """é¡¯ç¤ºå´é‚Šæ¬„é¸æ“‡åŠŸèƒ½"""
        st.sidebar.title("æ“ä½œé¸æ“‡")
        options = [opt[0] for opt in self.OPTIONS.values()]
        selected_mode = next(
            key for key, value in self.OPTIONS.items() if value[0] == st.sidebar.radio("è«‹é¸æ“‡æ“ä½œæ¨¡å¼ï¼š", options, key="mode_selection"))
        uploaded_file = st.sidebar.file_uploader("è«‹ä¸Šå‚³æª”æ¡ˆï¼š", type=self.OPTIONS[selected_mode][1])
        summary_prompt = st.sidebar.text_area("è‡ªè¨‚æ‘˜è¦æç¤ºèªï¼š", "æ ¹æ“šä»¥ä¸‹å°è©±å…§å®¹ç”Ÿæˆæ‘˜è¦ï¼Œä¸¦åŠ ä¸Šæ¨™é¡Œï¼š{context}")
        is_submitted = st.sidebar.button("é–‹å§‹åŸ·è¡Œ")
        return selected_mode, uploaded_file, summary_prompt, is_submitted


if __name__ == "__main__":
    MeetingSummaryApp().run()
